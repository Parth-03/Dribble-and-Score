import numpy as np
import random
import matplotlib.pyplot as plt
from matplotlib.patches import Circle, Rectangle
from FootballMDP import FootballSimulatorMDP

class TrueOnlineSarsaVisualizer:
    def __init__(self, env, alpha=0.1, gamma=0.99, lambda_=0.9, epsilon=0.1, num_episodes=1000):
        self.env = env
        self.alpha = alpha
        self.gamma = gamma
        self.lambda_ = lambda_
        self.epsilon = epsilon
        self.num_episodes = num_episodes
        self.q_table = {}
        self.eligibility_traces = {}

        self.v = np.array([[0.0 for j in range(self.env.grid_size[0])] for i in range(self.env.grid_size[1])])
        self.policy = np.array([[[0.25 for k in range(5)]for j in range(self.env.grid_size[0])] for i in range(self.env.grid_size[1])])
        self.q = np.array([[[0.0 for a in range(5)] for j in range(self.env.grid_size[0])] for i in range(self.env.grid_size[1])])
        
    def get_q_value(self, state, action):
        if (state, action) not in self.q_table:
            self.q_table[(state, action)] = 0.0
        return self.q_table[(state, action)]
    
    def set_q_value(self, state, action, value):
        self.q_table[(state, action)] = value
    
    def get_trace(self, state, action):
        if (state, action) not in self.eligibility_traces:
            self.eligibility_traces[(state, action)] = 0.0
        return self.eligibility_traces[(state, action)]
    
    def set_trace(self, state, action, value):
        self.eligibility_traces[(state, action)] = value

    def epsilon_greedy_policy(self, state):
        actions = ['up', 'down', 'left', 'right', 'shoot']
        if np.random.uniform() < self.epsilon:
            return random.choice(actions)
        else:
            q_values = [self.get_q_value(state, a) for a in actions]
            # q_values = [q[state][self.get_action_index(a)] for a in actions]
            return actions[np.argmax(q_values)]

    def get_action_index(self, action):
        a= 0
        if action == 'up':
            a =0
        elif action == 'down':
            a =1
        elif action == 'left':
            a=2
        elif action == 'right':
            a=3
        elif action == 'shoot':
            a=4       
        return a   
    
    
    def train(self):
        reward_data = []
        episode_data = []
        for episode in range(self.num_episodes):
            if episode%500 ==0:
                self.epsilon = self.epsilon/2
            state = self.env.reset()
            action = self.epsilon_greedy_policy(state)
            self.eligibility_traces.clear()
            G = 0
            while True:
                next_state, reward, done, _ = self.env.step(action)
                next_action = self.epsilon_greedy_policy(next_state)
                G+= reward
                
                q_current = self.get_q_value(state, action)
                q_next = self.get_q_value(next_state, next_action)
                td_error = reward + self.gamma * q_next - q_current
                # q_value = q_current + self.alpha * td_error
                # self.set_q_value(state, action, q_value)
                self.set_trace(state, action, self.get_trace(state, action) + 1.0)
                
                for (s, a), trace_value in self.eligibility_traces.items():
                    q_value = self.get_q_value(s, a)
                    q_value += self.alpha * td_error * trace_value
                    self.set_q_value(s, a, q_value)
                    self.set_trace(s, a, self.gamma * self.lambda_ * trace_value)
                
                if done:
                    break
                
                state, action = next_state, next_action
            
            if (episode + 1) % 100 == 0:
                print(f"Episode {episode + 1}/{self.num_episodes} completed.")
            
            reward_data.append(G)
            episode_data.append(episode)

        plt.scatter(episode_data, reward_data)
        plt.show()

    
    def get_greedy_policy(self):
        """Get the greedy policy for the entire grid."""
        policy_grid = np.full(self.env.grid_size, '', dtype='<U10')
        for x in range(self.env.grid_size[0]):
            for y in range(self.env.grid_size[1]):
                state = (x, y)  # Mock opponents for visualization
                actions = ['up', 'down', 'left', 'right', 'shoot']
                q_values = [self.get_q_value(state, action) for action in actions]
                best_action = actions[np.argmax(q_values)]
                policy_grid[x, y] = best_action
        return policy_grid

    def visualize_policy(self):
        """Visualize the greedy policy on the grid."""
        policy_grid = self.get_greedy_policy()
        print(policy_grid)

        fig, ax = plt.subplots(figsize=(12, 6))
        ax.set_title("Greedy Policy Visualization")
        ax.set_xlim(0, self.env.grid_size[0])
        ax.set_ylim(0, self.env.grid_size[1])
        ax.set_xticks(range(self.env.grid_size[0]))
        ax.set_yticks(range(self.env.grid_size[1]))
        ax.grid(False)

        for x in range(self.env.grid_size[0]):
            for y in range(self.env.grid_size[1]):
                action = policy_grid[x, y]
                if (x,y) in self.env.opponents:
                    ax.text(x, y, ' ', ha='center', va='center', fontsize=10)
                elif action == 'up':
                    ax.text(x, y, '↑', ha='center', va='center', fontsize=20)
                elif action == 'down':
                    ax.text(x, y, '↓', ha='center', va='center', fontsize=20)
                elif action == 'left':
                    ax.text(x, y, '←', ha='center', va='center', fontsize=20)
                elif action == 'right':
                    ax.text(x, y, '→', ha='center', va='center', fontsize=20)
                elif action == 'shoot':
                    ax.text(x, y, 'SHOOT!', ha='center', va='center', fontsize=10)

        plt.show()
    
    def visualize_episode(self, show_trajectory=True):
        state = self.env.reset()
        print("Initial State:", state)
        trajectory = [state]
        
        plt.figure(figsize=(10, 5))
        ax = plt.gca()
        ax.set_xlim(0, self.env.grid_size[0])
        ax.set_ylim(0, self.env.grid_size[1])
        ax.set_aspect('equal')
        ax.set_title("Football Simulator - Agent Movement")
        plt.xlabel("X")
        plt.ylabel("Y")
        
        # Draw the goal
        goal_x, goal_y = self.env.goal
        ax.add_patch(Rectangle((goal_x-0.1, goal_y -0.1), 0.5, 0.5, color='gold', alpha=0.5, label="Goal"))

        # Draw opponents
        for opp_x, opp_y in self.env.opponents:
            ax.add_patch(Circle((opp_x, opp_y), 0.1, color='red', alpha=0.7, label="Opponent"))

        # agent_x, agent_y = state
        # ax.add_patch(Circle((agent_x, agent_y), 0.1, color='blue', label="Agent"))

        # plt.legend(loc="upper left")
        # plt.show()

        done = False
        while not done:
            action = self.epsilon_greedy_policy(state)
            state, reward, done, info = self.env.step(action)
            if len(info)!=0:
                print(f"Action: {action}, State: {state}, Reward: {reward}, XG: {info} Done: {done}")
            else:
                print(f"Action: {action}, State: {state}, Reward: {reward}, Done: {done}")
            #ax.plot(state, color='blue', linewidth=1, alpha=0.7)
            trajectory.append(state)

        # Draw the trajectory
        if show_trajectory:
            for i in range(1, len(trajectory)):
                x1, y1 = trajectory[i - 1]
                x2, y2 = trajectory[i]
                ax.plot([x1, x2], [y1, y2], color='blue', linewidth=2, alpha=0.7)

        # Draw the agent's final position
        agent_x, agent_y = state
        ax.add_patch(Circle((agent_x, agent_y), 0.1, color='blue', alpha = 0.7, label="Agent"))

        plt.legend(loc="upper left")
        plt.show()

# Example usage
env = FootballSimulatorMDP()

agent = TrueOnlineSarsaVisualizer(env, alpha=0.1, gamma=0.9, lambda_=0.9, epsilon=0.5, num_episodes=10000)

# Train the agent
for _ in range(1):
    #agent.sarsa(alpha=0.01, eps=0.1)
    agent.train()

agent.visualize_policy()

# Visualize an episode
for _ in range(5):
    agent.visualize_episode(show_trajectory=True)
